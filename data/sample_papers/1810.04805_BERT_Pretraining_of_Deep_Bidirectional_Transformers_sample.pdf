%PDF-1.4
1 0 obj
<< /Type /Catalog /Pages 2 0 R >>
endobj
2 0 obj
<< /Type /Pages /Kids [3 0 R] /Count 1 >>
endobj
3 0 obj
<< /Type /Page /Parent 2 0 R /MediaBox [0 0 612 792] /Resources << /Font << /F1 4 0 R >> >> /Contents 5 0 R >>
endobj
4 0 obj
<< /Type /Font /Subtype /Type1 /BaseFont /Helvetica >>
endobj
5 0 obj
<< /Length 409 >>
stream
BT
/F1 11 Tf
50 760 Td
14 TL
(Paper: BERT \(sample notes\)) Tj
T* (BERT pretrains deep bidirectional transformers using masked language modeling.) Tj
T* (It uses next sentence prediction during pretraining and then task-specific fine-tuning.) Tj
T* (The approach significantly improved benchmarks in QA and language understanding tasks.) Tj
T* (This sample file is included for quick local RAG testing.) Tj
ET
endstream
endobj
xref
0 6
0000000000 65535 f 
0000000009 00000 n 
0000000058 00000 n 
0000000115 00000 n 
0000000241 00000 n 
0000000311 00000 n 
trailer
<< /Size 6 /Root 1 0 R >>
startxref
771
%%EOF
