%PDF-1.4
1 0 obj
<< /Type /Catalog /Pages 2 0 R >>
endobj
2 0 obj
<< /Type /Pages /Kids [3 0 R] /Count 1 >>
endobj
3 0 obj
<< /Type /Page /Parent 2 0 R /MediaBox [0 0 612 792] /Resources << /Font << /F1 4 0 R >> >> /Contents 5 0 R >>
endobj
4 0 obj
<< /Type /Font /Subtype /Type1 /BaseFont /Helvetica >>
endobj
5 0 obj
<< /Length 439 >>
stream
BT
/F1 11 Tf
50 760 Td
14 TL
(Paper: Attention Is All You Need \(sample notes\)) Tj
T* (The paper introduces the Transformer architecture based on self-attention.) Tj
T* (It replaces recurrence with multi-head attention and position-wise feed-forward blocks.) Tj
T* (Key benefits include parallel training, strong sequence modeling, and improved translation quality.) Tj
T* (This sample file is included for quick local RAG testing.) Tj
ET
endstream
endobj
xref
0 6
0000000000 65535 f 
0000000009 00000 n 
0000000058 00000 n 
0000000115 00000 n 
0000000241 00000 n 
0000000311 00000 n 
trailer
<< /Size 6 /Root 1 0 R >>
startxref
801
%%EOF
